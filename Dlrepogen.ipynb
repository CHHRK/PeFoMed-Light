{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
    
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes accelerate transformers peft huggingface_hub tqdm pandas==2.2.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_-JnlsEmR91",
        "outputId": "a1630f8d-795f-4da3-c679-3a70485bdf1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "\n",
        "DATA_PATH    = \"/content/DL/train.csv\"              # CheXpert dataset\n",
        "INDIANA_PATH = \"/content/DL/indiana_reports.csv\"    # Indiana reports (few-shot)\n",
        "OUTPUT_FILE  = \"/content/DL/final_reports_tinyllama_fewshot_10k.csv\"\n",
        "\n",
        "START_INDEX   = 45000\n",
        "END_INDEX     = 46000          # generate 10 000 samples\n",
        "BATCH_SIZE    = 2              # safe for Colab T4\n",
        "MAX_NEW_TOKENS = 100           # ‚âà 50 words\n",
        "TEMPERATURE   = 0.6\n",
        "TOP_P         = 0.9\n",
        "\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "print(\"‚úÖ Configuration ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRzva4yumR66",
        "outputId": "5cb93b78-7dac-4cc6-da44-f2ac31815ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Configuration ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"üîÑ Loading model: {MODEL_NAME}\")\n",
        "\n",
        "use_bnb = True\n",
        "try:\n",
        "    if use_bnb:\n",
        "        bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\"\n",
        "        ).eval()\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\").eval()\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è Quantized load failed, retrying non-quantized load:\", e)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\").eval()\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "print(\"‚úÖ Model + tokenizer loaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397,
          "referenced_widgets": [
            "688c781dcf224426abf74e2104b84bd1",
            "8fb080bc3b9143599d19e1eaa8aed3f8",
            "b8de734d53cf4cf9b50a97549f5dca39",
            "d53f2b890e414308b02b02422aa13066",
            "838ba59fac83410d815d20c75d058de5",
            "1ec01f77c6bb444c954d9e40bd160745",
            "9995e79adf6c4d5bac3afa0927c58411",
            "de09c57dae274f44bf4d9b02508b9430",
            "cbdc4cc7895e49169ceda38bffa1edc7",
            "8e994b3796294c799e2fc9593bd52697",
            "68cf4a4d5d2b43c2aecc9f5b3c8c8237",
            "612c4f3c6a7b4e018f7d36d63046b47a",
            "9ed9ac7363734e7792637c5bc7b5848b",
            "48d49da608fa4e979d5c9c3cfaf18704",
            "46e4a725250548a5926b24a7321a3e3c",
            "bd6eafe742324e4eb3bc8962b6fe041a",
            "c669b32cc9774f1c94992a9c903adfad",
            "9111a7e5bd8a46a8abd1a077ce25c6ae",
            "a92bc4e3d24c4554a093bf6826c79658",
            "f6793b2223ec4b599a352160d43a184e",
            "a9fb5f9132d74d4fafd5728606bbb2a3",
            "9c42e7b80eaf480e9a3e4148ec3f4a61",
            "8cac1da6ebaf4b3188604cb6e4bbe8c6",
            "e063ccf0478a41249b03141f01a2ee0e",
            "1d552e3573ba45ff8fd6b21234eca4ca",
            "b8c3a64b3bcd4d08a350daffc8d7c710",
            "5eefe216dd4f496c8b6fd674cf73baf9",
            "38c68c99e7194919b61bf78f9d897ba6",
            "41f513c862dc41c0bf3998ec556a3be8",
            "7916ef74c8bf4b1483b9b1857b1aa1bb",
            "ea5a42fd75aa436280ab0d49ec432add",
            "051e97aeee7c4172a24ac0a7f4cbd51b",
            "66f064eb8a1c4508963e0fc9bc1d2ea8",
            "970efad22a4944c992cae79ab27c44a8",
            "9dcc963fb09f4a1ab38fd023a45007cc",
            "2ad63cce9c4f4039b58e6c0bf3d2e01e",
            "4b541506f1184bb48e6eb91c920baedc",
            "12858dadd10e4208b1465a1a0d9cb8d0",
            "ced202361eac4098b4a43e4db6e009a8",
            "051195ac4ffc49b2b4f82d732ce9ad45",
            "d28989fe38e94063a765db76ec93a604",
            "9919349288e442f28e5fc74619882758",
            "c2ff8e8ca3e143c0b083180e5df46d7f",
            "99fb59d3b739498c8aad41083ce92536",
            "1c64ca8e583f46669cd4d38bfce3ddec",
            "a21cd59f3c024b2284a888f138904737",
            "dc48e3f1767d46d0914a3655e881adf9",
            "a31d16dea30e4bd0ac5e4967e494dc54",
            "1f04054bc73340cc94ceff5873d72e95",
            "c1c6dab763a2489bbadc594c96e77555",
            "5c2809517b4c434893b2067a4cd16632",
            "ce1abf54c048439e89c39e4e823d230b",
            "afe2399c59da4d2eb8ea9492c21fc0d0",
            "6c8e955769f142da8a6129a0051f1113",
            "aa5623e90b0d48e9afd3067d2df41fb9",
            "981cbe74e4154fa98f14b6e875bd5036",
            "936053edd95f4936ac7a4c36ebca0881",
            "ea13bbb4aaa64f26bb8416f686a5e75c",
            "c443780f4a8541ac85a2293bee0eb4a5",
            "fa704e916fa741b2991139a027ba33ed",
            "9b55f8b1ebe847d79b4f1bf009c33f93",
            "45c2a9ddf30c435e8aef8b86f71624ae",
            "e120dc05d31d40a6947293c55674ccea",
            "aeaeeb09374c465d9975d3a5cb40be70",
            "d962118b561c4003bfc074be681acf85",
            "1a7292aeeb004508b0eb582a34ff91f5",
            "337dcc7f4a63430b950012fa72b6c308",
            "7d0e251929904441a22b1db782911b97",
            "2e446969ccd144b79b5dd86dc9558a39",
            "a4476c76b09c499e884c84fb22b36d58",
            "069d0d8e10af403eb6ee43ba07fc37c1",
            "7c8a13a7406b40e8a23d933691683d53",
            "26921b08057b48e49f55a8766631600c",
            "9c8d763d5b8845fea7ce778dd5d2382a",
            "1056c3b0640345868c287e77c09b4722",
            "22f528fb950a48fc9cccf0acea96d5d2",
            "12d240ed96ef47f297b4cab114be46de"
          ]
        },
        "id": "NZjhas9BmR4J",
        "outputId": "f73b6c63-5055-49ae-a5ec-645556931270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "688c781dcf224426abf74e2104b84bd1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "612c4f3c6a7b4e018f7d36d63046b47a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8cac1da6ebaf4b3188604cb6e4bbe8c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "970efad22a4944c992cae79ab27c44a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c64ca8e583f46669cd4d38bfce3ddec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "981cbe74e4154fa98f14b6e875bd5036"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "337dcc7f4a63430b950012fa72b6c308"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model + tokenizer loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chexpert_df = pd.read_csv(DATA_PATH, engine=\"python\", on_bad_lines=\"skip\", quoting=3)\n",
        "iuxray_df   = pd.read_csv(INDIANA_PATH)\n",
        "\n",
        "chexpert_df = chexpert_df.iloc[START_INDEX:END_INDEX].reset_index(drop=True)\n",
        "print(f\"‚úÖ CheXpert rows: {len(chexpert_df)} | IU-Xray rows: {len(iuxray_df)}\")\n",
        "\n",
        "label_cols = chexpert_df.columns[chexpert_df.columns.get_loc(\"No Finding\"):].tolist()\n",
        "print(\"üßπ Label columns:\\n\", label_cols)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jtup_YRWmR2S",
        "outputId": "664364ae-b1b7-4ce1-9904-7f64546685a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ CheXpert rows: 1000 | IU-Xray rows: 3851\n",
            "üßπ Label columns:\n",
            " ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_format_labels(row):\n",
        "    parts = []\n",
        "    for col in label_cols:\n",
        "        val = row.get(col)\n",
        "        if pd.isna(val):\n",
        "            parts.append(f\"{col}: Not Mentioned\")\n",
        "        elif val == 1.0:\n",
        "            parts.append(f\"{col}: Positive\")\n",
        "        elif val == 0.0:\n",
        "            parts.append(f\"{col}: Negative\")\n",
        "        elif val == -1.0:\n",
        "            parts.append(f\"{col}: Uncertain\")\n",
        "        else:\n",
        "            parts.append(f\"{col}: Not Mentioned\")\n",
        "    return \", \".join(parts)\n",
        "\n",
        "chexpert_df[\"formatted_labels\"] = chexpert_df.apply(safe_format_labels, axis=1)\n",
        "print(\"‚úÖ Labels formatted.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0xlX4_PmR1p",
        "outputId": "94b7ac36-7fa8-4a95-bb03-044e20f41087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Labels formatted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_uids = [1, 2, 4, 5, 6, 3995]\n",
        "example_findings = []\n",
        "for uid in example_uids:\n",
        "    try:\n",
        "        finding = iuxray_df.loc[iuxray_df[\"uid\"] == uid, \"findings\"].values[0]\n",
        "        example_findings.append(finding.replace('\"','').strip())\n",
        "    except:\n",
        "        example_findings.append(\"No acute cardiopulmonary abnormality.\")\n",
        "\n",
        "example_labels = [\n",
        "    \"No Finding: Positive, Enlarged Cardiomediastinum: Negative, Cardiomegaly: Negative, Lung Opacity: Negative, Pleural Effusion: Negative\",\n",
        "    \"No Finding: Negative, Cardiomegaly: Positive, Pleural Effusion: Positive\",\n",
        "    \"No Finding: Negative, Lung Opacity: Positive, Edema: Uncertain\",\n",
        "    \"No Finding: Negative, Pneumothorax: Positive, Support Devices: Positive\",\n",
        "    \"No Finding: Negative, Cardiomegaly: Uncertain, Lung Lesion: Uncertain\",\n",
        "    \"No Finding: Negative, Cardiomegaly: Positive, Lung Opacity: Positive, Pleural Effusion: Positive\"\n",
        "]\n",
        "\n",
        "few_shot_examples = [{\"labels\": l, \"findings\": f} for l, f in zip(example_labels, example_findings)]\n",
        "print(f\"‚úÖ Loaded {len(few_shot_examples)} few-shot examples.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye2CZKDSmRvX",
        "outputId": "8c955e48-de55-4b6c-b5b0-a55fa06e3f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 6 few-shot examples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_fewshot_prompt(input_labels):\n",
        "    example_texts = [f\"Input Labels:\\n{e['labels']}\\nFindings:\\n{e['findings']}\" for e in few_shot_examples]\n",
        "    examples_block = \"\\n\\n---\\n\\n\".join(example_texts)\n",
        "\n",
        "    prefix = (\n",
        "        \"You are an expert radiologist AI. Generate only the final concise and professional 'Findings' section \"\n",
        "        \"for a chest X-ray based solely on the structured labels below. \"\n",
        "        \"Do not repeat labels or instructions. Output must contain only the report text. \"\n",
        "        \"If all labels are Negative or Not Mentioned, respond exactly with 'No acute cardiopulmonary abnormality.' \"\n",
        "        \"Limit to ‚âà 50 words.\\n\\nHere are examples:\\n\\n\"\n",
        "    )\n",
        "\n",
        "    return f\"{prefix}{examples_block}\\n\\n---\\n\\nInput Labels:\\n{input_labels}\\nFindings:\"\n"
      ],
      "metadata": {
        "id": "MGd1vqDemlgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_output(text):\n",
        "    if \"Findings:\" in text:\n",
        "        text = text.split(\"Findings:\")[-1]\n",
        "    text = re.split(r\"---|Input Labels:|\\nInput Labels:\", text)[0]\n",
        "    return text.strip().strip('\"').strip()\n",
        "\n",
        "def generate_batch(prompts):\n",
        "    clean_prompts = [p.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\") for p in prompts]\n",
        "    inputs = tokenizer(\n",
        "        clean_prompts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=2048\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            temperature=TEMPERATURE,\n",
        "            top_p=TOP_P,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return [clean_output(t) for t in decoded]\n",
        "\n",
        "print(\"‚úÖ Generation function ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prEGLReLmlRh",
        "outputId": "49d17b07-7c77-4e9c-ae2d-a09da0ca1c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Generation function ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = chexpert_df.head(5)\n",
        "test_prompts = [build_fewshot_prompt(r[\"formatted_labels\"]) for _, r in test_df.iterrows()]\n",
        "print(\"üß™ Testing few-shot generation on 5 samples‚Ä¶\\n\")\n",
        "\n",
        "test_outputs = generate_batch(test_prompts)\n",
        "for i, out in enumerate(test_outputs, 1):\n",
        "    print(f\"ü©∫ SAMPLE {i}:\\n{out}\\n{'-'*80}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u-_WgXimlQ0",
        "outputId": "0d6236a7-a8e3-4d00-f9af-fa5fa1645ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing few-shot generation on 5 samples‚Ä¶\n",
            "\n",
            "ü©∫ SAMPLE 1:\n",
            "Negative, No acute cardiopulmonary abnormality. There is no XXXX of a pleural effusion. There is no XXXX of a pneumothorax. There is no XXXX of a fracture. There is no XXXX of support devices. There is no XXXX of a lung lesion. There is no XXXX of a pleural effusion. There is no XXXX of a pneumothorax. There is no XX\n",
            "--------------------------------------------------------------------------------\n",
            "ü©∫ SAMPLE 2:\n",
            "The cardiomediastinal silhouette and pulmonary vasculature are within normal limits in size. There is no pneumothorax or pleural effusion. There is no acute bony findings. There is no XXXX of a pleural effusion. There is no focal scarring or atelectasis in the right midlung. There is no XXXX of a pleural effusion. There are no acute cardiopulmonary\n",
            "--------------------------------------------------------------------------------\n",
            "ü©∫ SAMPLE 3:\n",
            "There is no acute cardiomegaly, no cardiomegaly, no lung opacities, no atelectasis, no pneumothorax, no pleural effusion, no fracture, no support devices, no lung lesions, no atelectasis, no pneumothorax, no pleural effusion, no fracture, no support devices, no lung lesions, no atelectasis, no pneumothor\n",
            "--------------------------------------------------------------------------------\n",
            "ü©∫ SAMPLE 4:\n",
            "The cardiomediastinal silhouette and pulmonary vasculature are within normal limits in size. The lungs are mildly hyperinflated with flattening of the diaphragms and expansion of the retrosternal clear space. Compared with prior exam, there has been interval resolution of previously demonstrated bibasilar infiltrates. There is no XXXX scarring or atelectasis in the right midlung. There is no XXXX foc\n",
            "--------------------------------------------------------------------------------\n",
            "ü©∫ SAMPLE 5:\n",
            "The cardiomediastinal silhouette and pulmonary vasculature are within normal limits. The lungs are mildly hyperinflated with flattening of the diaphragms and expansion of the retrosternal clear space. Compared with prior exam, there has been interval resolution of previously demonstrated bibasilar infiltrates. There is minimal XXXX scarring or atelectasis in the right midlung. There is no XXXX focal air\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_reports = []\n",
        "paths = chexpert_df.get(\"Path\", [None]*len(chexpert_df))\n",
        "total = len(chexpert_df)\n",
        "\n",
        "print(f\"üöÄ Starting full generation for {total} samples‚Ä¶\")\n",
        "\n",
        "for start in tqdm(range(0, total, BATCH_SIZE), desc=\"‚ö° Generating Reports\"):\n",
        "    end = min(start + BATCH_SIZE, total)\n",
        "    batch = chexpert_df.iloc[start:end]\n",
        "    prompts = [build_fewshot_prompt(r[\"formatted_labels\"]) for _, r in batch.iterrows()]\n",
        "\n",
        "    try:\n",
        "        outs = generate_batch(prompts)\n",
        "        generated_reports.extend(outs)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error batch {start}-{end}: {e}\")\n",
        "        generated_reports.extend([\"Error generating report\"] * len(batch))\n",
        "\n",
        "    # üíæ Autosave every 200 samples\n",
        "    if (end % 200 == 0) or (end == total):\n",
        "        pd.DataFrame({\n",
        "            \"Path\": paths[:len(generated_reports)],\n",
        "            \"Report_Impression\": generated_reports\n",
        "        }).to_csv(OUTPUT_FILE, index=False)\n",
        "        print(f\"üíæ Progress saved at {end}/{total}\")\n",
        "\n",
        "print(f\"\\nüéâ Completed {len(generated_reports)} samples!\")\n",
        "print(f\"üìÅ Final output: {OUTPUT_FILE}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssMY-qgCmvZB",
        "outputId": "6fea6c3b-500b-49e4-d587-dfb14e341e39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting full generation for 1000 samples‚Ä¶\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "‚ö° Generating Reports:  20%|‚ñà‚ñà        | 100/500 [15:31<1:02:09,  9.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Progress saved at 200/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "‚ö° Generating Reports:  40%|‚ñà‚ñà‚ñà‚ñà      | 200/500 [31:14<45:51,  9.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Progress saved at 400/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "‚ö° Generating Reports:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 300/500 [46:37<30:38,  9.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Progress saved at 600/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "‚ö° Generating Reports:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 400/500 [1:01:58<15:22,  9.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Progress saved at 800/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "‚ö° Generating Reports: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [1:17:13<00:00,  9.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Progress saved at 1000/1000\n",
            "\n",
            "üéâ Completed 1000 samples!\n",
            "üìÅ Final output: /content/DL/final_reports_tinyllama_fewshot_10k.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_df = pd.read_csv(OUTPUT_FILE)\n",
        "print(f\"‚úÖ Generated {len(out_df)} reports!\\n\")\n",
        "print(out_df.sample(5))\n"
      ],
      "metadata": {
        "id": "UiWWNLf7mvYW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02e84762-8dd6-40b3-bc63-c9684f1aa884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Generated 1000 reports!\n",
            "\n",
            "                                       Path  \\\n",
            "265   patient11024/study1/view2_lateral.jpg   \n",
            "730  patient11139/study10/view1_frontal.jpg   \n",
            "178   patient11015/study1/view1_frontal.jpg   \n",
            "981   patient11198/study4/view1_frontal.jpg   \n",
            "852   patient11169/study1/view1_frontal.jpg   \n",
            "\n",
            "                                     Report_Impression  \n",
            "265  There are diffuse bilateral interstitial and a...  \n",
            "730  The cardiomediastinal silhouette and pulmonary...  \n",
            "178  There is no evidence of acute cardiopulmonary ...  \n",
            "981  The cardiomediastinal silhouette and pulmonary...  \n",
            "852  There is diffuse bilateral interstitial and al...  \n"
          ]
        }
      ]
    }
  ]
}